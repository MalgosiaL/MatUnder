{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Underpinnings - Lab 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "### a) Generative approach \n",
    "\n",
    "We know $\\pi = P(Y=1)$ and the distributions $f(x|Y=1)$ and $f(x|Y=-1)$.\n",
    "\n",
    "#### First bullet point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.54748832,  1.26550335],\n",
       "        [ 0.92715183,  1.14883792],\n",
       "        [ 0.07907533, -0.75198287],\n",
       "        [-1.24471967,  0.53543659],\n",
       "        [ 0.80680731,  1.40046828]]),\n",
       " array([0, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling from multivariate normal distribution and from Bernoulli\n",
    "\n",
    "x = np.random.multivariate_normal(np.array([0,0]), np.eye(2), 5)\n",
    "y = np.random.binomial(1, 0.5, 5)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.array([1,1])\n",
    "m2 = np.array([0,0])\n",
    "\n",
    "sigma = np.array([[1, -0.5],[-0.5, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "# Y_generative = \n",
    "# X_generative = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the distribution of $P(Y=1|X=x)=p(y=1|x)$ correspond to a logistic model?\n",
    "\n",
    "A hint: Use Bayes theorem to compute $p(y=1|x)$. Is it possible to represent $p(y=1|x)$ as $\\frac{e^{\\beta_0 + \\beta x}}{1 + e^{\\beta_0 + \\beta x}}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second bullet point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the formulas for the parameters of the logistic model (the coefficients and the intercept).\n",
    "\n",
    "A hint: Use the representation of $p(y=1|x)$ from the first bullet point and solve for $\\beta_0$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing beta_0 and beta using the formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a logistic model\n",
    "# mod_a = LogisticRegression(penalty=None)\n",
    "# mod_a.fit(X_generative, Y_generative)\n",
    "# (mod_a.intercept_, mod_a.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Discriminative approach\n",
    "\n",
    "We know $f(x)$ and $P(Y=1|X=x)$.\n",
    "\n",
    "#### First bullet point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "# X_disciminative = \n",
    "# Y_discriminative = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod_b = LogisticRegression(penalty=None)\n",
    "# mod_b.fit(X_disciminative, Y_discriminative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second bullet point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(..., c=Y_generative)\n",
    "# plt.ylim(-4,4)\n",
    "# plt.xlim(-4,4)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(... , c=Y_discriminative)\n",
    "# plt.ylim(-4,4)\n",
    "# plt.xlim(-4,4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What distinguishes the generative approach from the discriminative approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from $f_{X|Y=-1}$, first, we will give an answer to Q1.\n",
    "\n",
    "Q1. A hint: use Bayes theorem for $p(x|y=-1)$ and use law of total probability for $p(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "\n",
    "# n1 = ...\n",
    "# x1 = np.random.exponential(1/lambda, size=n1)\n",
    "\n",
    "# n2 = ...\n",
    "# x2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model\n",
    "# LogisticRegression(penalty=None)\n",
    "# mod.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2, Q3. A hint: what is the distribution of $f_{X|Y=-1}$? What ia a norming constant?\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that is doable, given $\\beta_1$ and $\\pi$ compute $\\beta_0$.\n",
    "\n",
    "A hint: Of course it is, compute $\\beta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta1 = 1\n",
    "# pi_c = n1/(n1 + n2)\n",
    "# beta0 = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
