{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Underpinnings - Lab 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "### a) Generative approach \n",
    "\n",
    "We know $\\pi = P(Y=1)$ and the distributions $f(x|Y=1)$ and $f(x|Y=-1)$.\n",
    "\n",
    "#### First bullet point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.8357655 ,  0.44902948],\n",
       "        [ 0.34352241, -0.88662649],\n",
       "        [ 0.71461253, -0.52791778],\n",
       "        [ 0.84917764,  1.53720319],\n",
       "        [-0.24184231, -0.64272448]]),\n",
       " array([0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling from multivariate normal distribution and from Bernoulli\n",
    "\n",
    "x = np.random.multivariate_normal(np.array([0,0]), np.eye(2), 5)\n",
    "y = np.random.binomial(1, 0.5, 5)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.array([1,1])\n",
    "m2 = np.array([0,0])\n",
    "\n",
    "sigma = np.array([[1, -0.5],[-0.5, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "Y_generative = np.concatenate((np.ones(500),np.ones(500)*-1))\n",
    "X_generative = np.concatenate((\n",
    "    np.random.multivariate_normal(mean=m1, cov=sigma, size=500),\n",
    "    np.random.multivariate_normal(mean=m2, cov=sigma, size=500)\n",
    "),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_generative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.01569547]\n",
      "[[1.89526077 2.08510365]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(fit_intercept=True)\n",
    "lr.fit(X_generative,Y_generative)\n",
    "print(lr.intercept_)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n",
      "[2. 2.]\n"
     ]
    }
   ],
   "source": [
    "b0 =  (m2.T @ np.linalg.inv(sigma) @ m2 - m1.T @ np.linalg.inv(sigma) @ m1)/2\n",
    "print(b0)\n",
    "b = np.linalg.inv(sigma) @ (m1-m2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roughly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the distribution of $P(Y=1|X=x)=p(y=1|x)$ correspond to a logistic model?\n",
    "\n",
    "A hint: Use Bayes theorem to compute $p(y=1|x)$. Is it possible to represent $p(y=1|x)$ as $\\frac{e^{\\beta_0 + \\beta x}}{1 + e^{\\beta_0 + \\beta x}}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second bullet point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the formulas for the parameters of the logistic model (the coefficients and the intercept).\n",
    "\n",
    "A hint: Use the representation of $p(y=1|x)$ from the first bullet point and solve for $\\beta_0$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing beta_0 and beta using the formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a logistic model\n",
    "# mod_a = LogisticRegression(penalty=None)\n",
    "# mod_a.fit(X_generative, Y_generative)\n",
    "# (mod_a.intercept_, mod_a.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Discriminative approach\n",
    "\n",
    "We know $f(x)$ and $P(Y=1|X=x)$.\n",
    "\n",
    "#### First bullet point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "# first generate x\n",
    "# mix = np.random.binomial(1,p=0.5,size=1000)\n",
    "# X1 = np.random.multivariate_normal(mean=m1, cov=sigma, size=1000)\n",
    "# X2 = np.random.multivariate_normal(mean=m2, cov=sigma, size=1000)\n",
    "# # X_disciminative = \n",
    "# np.where?\n",
    "\n",
    "# Y_discriminative = np.random.binomial(-2, -1/(1+np.exp(-2-np.array([-2,-2])*X_disciminative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what distinguishes - in generative we can choose how much n in classes, and in discriminative we can't "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod_b = LogisticRegression(penalty=None)\n",
    "# mod_b.fit(X_disciminative, Y_discriminative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second bullet point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(..., c=Y_generative)\n",
    "# plt.ylim(-4,4)\n",
    "# plt.xlim(-4,4)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(... , c=Y_discriminative)\n",
    "# plt.ylim(-4,4)\n",
    "# plt.xlim(-4,4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What distinguishes the generative approach from the discriminative approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from $f_{X|Y=-1}$, first, we will give an answer to Q1.\n",
    "\n",
    "Q1. A hint: use Bayes theorem for $p(x|y=-1)$ and use law of total probability for $p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have: \n",
    "$$\n",
    "p(y=1|x) = \\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}} \\quad (*)\n",
    "$$\n",
    "and we know that: $P(X=x|Y=1) = e^{-x}$.\n",
    "\n",
    "From Bayes:\n",
    "$$\n",
    "P(y=1|x) = \\frac{\\pi e^{-x}}{\\pi e^{-x} + (1-\\pi)p(x|y=-1)}\n",
    "$$\n",
    "\n",
    "Therefore from (*):\n",
    "$$\n",
    "p(x|y=-1)=\\frac{\\frac{\\pi}{1-\\pi}e^{-\\beta_0}}{(1+\\beta_1)}[(\\beta_1+1)e^{-(\\beta_1+1)x}] \\sim cEXP(1+\\beta_1)\n",
    "$$\n",
    "Which is the answer to __Q1__.\n",
    "\n",
    "__Q2__: Yes, as long as we adjust $\\beta_1$ so that the the pdf integrates to 1 (c = 1).\n",
    "\n",
    "__Q3__: It's not.\n",
    "\n",
    "__SOLUTION__:\n",
    "$$\n",
    "c = 1 \\implies \\beta_0 = -ln(\\frac{(\\beta+1)(1-\\pi)}{\\pi})\n",
    "$$\n",
    "Therefore for $n_1 = 1000$, $n_1 = 2000$, $\\beta_1 = 1$, we have $\\pi = \\frac{1}{3}$  \n",
    "$$\n",
    "\\beta_0 = -ln(4)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3862943611198906"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing beta_0 assuming that n_1 = 1000, n_2 = 2000 and b_1 = 1\n",
    "-np.log(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if the formula is correct\n",
    "b1 = 1\n",
    "\n",
    "n1 = 1000\n",
    "x1 = np.random.exponential(1, size=n1)\n",
    "\n",
    "n2 = 2000\n",
    "x2 = np.random.exponential(1/(1+b1), size=n2)\n",
    "\n",
    "y = np.concatenate((np.ones(n1), np.zeros(n2)))\n",
    "X = np.concatenate((x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.37690256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty=None)\n",
    "lr.fit(X.reshape(-1,1),y)\n",
    "lr.intercept_\n",
    "#close enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2, Q3. A hint: what is the distribution of $f_{X|Y=-1}$? What ia a norming constant?\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that is doable, given $\\beta_1$ and $\\pi$ compute $\\beta_0$.\n",
    "\n",
    "A hint: Of course it is, compute $\\beta_0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "$R(a,a^*) = \\mathbb{E} \\mathcal{L}(f(X), Y) = \\mathbb{E}(aX - Y)^2 = ...$,\n",
    "\n",
    "In our task we know $a^* = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def population_risk(a, sigma_eps):\n",
    "#     return ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical risk: $\\frac{1}{n} \\sum_{i=1}^n (ax_i - y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def empirical_risk(a, x, y):\n",
    "#     return ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code, plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excess risk: $$E(\\hat a, a^*) =R(\\hat a, a^*) - \\textrm{inf}_{a \\in A_0} R(a, a^*)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excess risk\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations with fixed sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations for various sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a nice plot visualising the results (how the unconditional excess risk changes with a sample size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
