{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, InputLayer, concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# scale the data\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix into vector\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "\n",
    "From the lecture we know that ELBO equals\n",
    "\n",
    "$$ELBO = \\mathbb{E}_q \\log p(x|z) - KL(q(z|x)||p(z)).$$\n",
    "\n",
    "Assume that we already know the parameters of the encoder \n",
    "\n",
    "$$\\mathbb{E}_q \\log p(x|z) = \\mathbb{E}_{\\varepsilon} \\log p(x|z_{\\phi}(\\varepsilon, x))$$\n",
    "\n",
    "Below: \n",
    "\n",
    " - `x` - observed vector of pixels (0 and 1)\n",
    " - `x_decoded` - $p(1|z_{\\phi}(\\varepsilon, x)) = p(1|\\mu_{\\phi}(x) + \\Sigma_{\\phi}(x)^{1/2}\\varepsilon)$, where $\\varepsilon$ was generated according to $\\mathcal{N}(0, 1)$ - the aposteriori probability of the pixel being $1$ after reparametrization trick\n",
    " - $\\Sigma_{\\phi}(x) = \\textrm{diag}(\\sigma_{\\phi, 1}^2(x), \\sigma_{\\phi, 2}^2, \\ldots, \\sigma_{\\phi, K}^2)$\n",
    " - `z_mean` - $\\mu_{\\phi}(x) = (\\mu_{\\phi, 1}(x), \\mu_{\\phi, 2}(x), \\ldots, \\mu_{\\phi, K}(x))$\n",
    " - `z_log_var` - $(\\log\\sigma_{\\phi, 1}^2(x), \\log\\sigma_{\\phi, 2}^2(x), \\ldots, \\log\\sigma_{\\phi, K}^2(x))$\n",
    "\n",
    " - $q(z|x) = \\mathcal{N}(\\mu_{\\phi}(x), \\Sigma_{\\phi}(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: `x, x_decoded, z_mean, z_log_var`\n",
    "\n",
    "Output: ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def elbo_vae(x, x_decoded, z_mean, z_log_var):\n",
    "    pass\n",
    "    # loss_term = bernoulli loss between true x and predicted x_decoded\n",
    "\n",
    "    # KL_term = KL divergence between normal with params z_mean, z_log_var and standard multivariate normal\n",
    "\n",
    "    # return tf.reduce_mean(loss_term - KL_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def elbo_vae(x, x_decoded, z_mean, z_log_var):\n",
    "\n",
    "    loss_term = tf.reduce_sum(x * tf.math.log(x_decoded + 1e-19) + (1 - x) * tf.math.log(1 - x_decoded + 1e-19), 1)\n",
    "\n",
    "    KL_term =  0.5 * tf.reduce_sum(-z_log_var + tf.exp(z_log_var) + tf.square(z_mean) - 1, 1)\n",
    "\n",
    "    return tf.reduce_mean(loss_term - KL_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "\n",
    "Next we need to get a sample from $q(z|x)$. We will use a reparametrization trick.\n",
    "\n",
    "Input: `z_mean, z_log_var`\n",
    "\n",
    "Output: $\\mathcal{N}(\\mu_{\\phi}(x), \\Sigma_{\\phi}(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        # epsilon = sample from standard normal distribution (sample batch observation from dim dimensional normal distribution)\n",
    "        return # transform epsilon so it becomes from the normal distribution with params z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "\n",
    "A part of the model that takes input and returns a vector:\n",
    "\n",
    "$$ (\\mu_{\\phi, 1}(x), \\mu_{\\phi, 2}(x), \\ldots, \\mu_{\\phi, K}(x), \\log\\sigma_{\\phi, 1}^2(x), \\log\\sigma_{\\phi, 2}^2(x), \\ldots, \\log\\sigma_{\\phi, K}^2(x)) $$\n",
    "\n",
    "Due to technical reason we will return the above vector and sampled values of $z$ as the output of the encoder (and not as a separate layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(original_dim))\n",
    "x = Dense(256, activation='relu')(encoder_inputs)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters:\n",
    "(784 + 1)*256, (256 + 1)*256, (256 + 1)*latent_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = Dense(256, activation='relu')(latent_inputs)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "decoder_outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            total_loss = -elbo_vae(data, reconstruction, z_mean, z_log_var)           \n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "\n",
    "        return {\"loss\": self.total_loss_tracker.result()}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(x_train, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representation in the latent space of observations from test ds\n",
    "z_test_pred = # encoder predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a list z_test_pred indeces mean: 0-vector of means, 1-vector of variances, 2-values in the latent space\n",
    "# this is a prediction\n",
    "test_pred = # decoder predictions - we take a vector from the latent space and transform it to be again a picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10  # figure with 10 x 2 digits\n",
    "digit_size = (np.sqrt(original_dim)).astype('int')\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "figure = np.zeros((digit_size * n, digit_size * 2))\n",
    "decoded = test_pred\n",
    "\n",
    "for i in range(10):\n",
    "    figure[i * digit_size: (i + 1) * digit_size, :digit_size] = x_test[i, :].reshape(digit_size, digit_size)\n",
    "    figure[i * digit_size: (i + 1) * digit_size, digit_size:] = decoded[i, :].reshape(digit_size, digit_size)\n",
    "    \n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use z_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_test_pred_tsne = tsne.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000\n",
    "sns.scatterplot(x=z_test_pred_tsne[:N,0], y=z_test_pred_tsne[:N,1], hue=y_test[:N], palette=sns.color_palette(\"tab10\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
